{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generation with Anthropic Claude 3, Amazon Bedrock and Llama-index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this notebook we will show you how to use Llama-index, Anthropic Claude 3 and Knowledge base for Amazon Bedrock to build a Retrieval Augmented Generation (RAG) solution\n",
    "\n",
    "\n",
    "#### Use case\n",
    "\n",
    "To demonstrate the RAG capability of Anthropic Claude 3, let's take the use case of an AI Assistant that can help answer questions from a personal document. \n",
    "\n",
    "\n",
    "#### Persona\n",
    "You are Bob,an Application Developer at Anycompany. Anycompany is experiencing an overwhelming number of customer queries. Anycompany has built a secure and performant conversational AI Assistant to answer frequently asked questions. Now Anycompany wants this conversational AI assistant to be able to answer questions are which are specific to the company. \n",
    "\n",
    "In this workshop, you will build a context aware conversational AI Assistant for Anycompany \n",
    "\n",
    "#### Implementation\n",
    "To fulfill this use case, in this notebook we will show how to create a RAG Application to answer questions from business data. We will use  Anthropic Claude 3 Sonnet Foundation model, Knowledge base for Amazon Bedrock and Llama-index. \n",
    "\n",
    "#### Prerequisites\n",
    "You should have ingested your documents in Knowledge Base for Amazon Bedrock by following the steps mentioned [here](https://docs.aws.amazon.com/bedrock/latest/userguide/knowledge-base.html)\n",
    "\n",
    "In this example, we have ingested the following documents in Knowledge Base for Amazon Bedrock:\n",
    "1. [2019 Shareholder letter](https://s2.q4cdn.com/299287126/files/doc_financials/2020/ar/2019-Shareholder-Letter.pdf)\n",
    "2. [2020 Shareholder letter](https://s2.q4cdn.com/299287126/files/doc_financials/2021/ar/Amazon-2020-Shareholder-Letter-and-1997-Shareholder-Letter.pdf')\n",
    "3. [2021 Shareholder letter](https://s2.q4cdn.com/299287126/files/doc_financials/2022/ar/2021-Shareholder-Letter.pdf)\n",
    "4. [2022 Shareholder letter](https://s2.q4cdn.com/299287126/files/doc_financials/2023/ar/2022-Shareholder-Letter.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Python 3.10\n",
    "\n",
    "⚠  For this lab we need to run the notebook based on a Python 3.10 runtime. ⚠\n",
    "\n",
    "\n",
    "## Installation\n",
    "\n",
    "To run this notebook you would need to install dependencies - llama-index and llama-index-llms-bedrock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%pip install llama-index --force-reinstall --quiet\n",
    "%pip install llama-index-llms-bedrock --force-reinstall --quiet\n",
    "%pip install llama-index-retrievers-bedrock --force-reinstall --quiet\n",
    "%pip install llama-index-embeddings-bedrock --force-reinstall --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel Restart\n",
    "\n",
    "Restart the kernel with the updated packages that are installed through the dependencies above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# restart kernel\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup \n",
    "\n",
    "Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from llama_index.llms.bedrock import Bedrock\n",
    "from llama_index.core.llms import ChatMessage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "\n",
    "Initiate Bedrock Runtime through llama_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "model_id = 'anthropic.claude-3-sonnet-20240229-v1:0' # change this to use a different version from the model provider\n",
    "\n",
    "llm = Bedrock(\n",
    "   model=model_id\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval\n",
    "\n",
    "Retrieve relevant documents from Knowledge Base for Amazon Bedrock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "from llama_index.retrievers.bedrock import AmazonKnowledgeBasesRetriever\n",
    "\n",
    "\n",
    "kb_id = getpass(\"Knowledge Bases for Amazon Bedrock ID: \")\n",
    "\n",
    "retriever = AmazonKnowledgeBasesRetriever(\n",
    "    knowledge_base_id=kb_id,\n",
    "    retrieval_config={\n",
    "        \"vectorSearchConfiguration\": {\n",
    "            \"numberOfResults\": 4,\n",
    "        }\n",
    "    },\n",
    ")\n",
    "\n",
    "query = \"What role did Amazon play during pandemic?\"\n",
    "retrieved_results = retriever.retrieve(query)\n",
    "\n",
    "# Prints the first retrieved result\n",
    "print(retrieved_results[0].get_content())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Invocation and Response Generation\n",
    "\n",
    "Invoke the model and visualize the response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import get_response_synthesizer\n",
    "\n",
    "response_synthesizer = get_response_synthesizer(\n",
    "    response_mode=\"compact\", llm=llm\n",
    ")\n",
    "response_obj = response_synthesizer.synthesize(query, retrieved_results)\n",
    "print(response_obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "You have now experimented with using `llama-index` SDK to get an exposure to Anthropic Claude 3 and Amazon Bedrock. Using llama-index you have generated an email responding to a customer due to their negative feedback.\n",
    "\n",
    "### Take aways\n",
    "- Adapt this notebook to experiment with different Claude 3 models available through Amazon Bedrock. \n",
    "- Change the prompts to your specific usecase and evaluate the output of different models.\n",
    "- Play with the token length to understand the latency and responsiveness of the service.\n",
    "- Apply different prompt engineering principles to get better outputs.\n",
    "\n",
    "## Thank You"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
